import torch
from torch import nn
from torch.nn import functional as F
import config as cfg

class Head(nn.Module):
    """One self attention head class"""

    def __init__(self, vocab_size, head_size):
        super.__init__()



if __name__ == '__main__':
    pass

