batch_size = 64
block_size = 16
device = 'cpu'
tokenizer_model = "gpt2"
n_embed = 32
dropout_rate = 0.2
num_heads = 4
n_blocks = 4
train_val_split = 0.8
epochs = 500